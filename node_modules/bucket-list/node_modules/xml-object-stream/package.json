{
  "author": {
    "name": "Sean Hess",
    "email": "sean@i.tv"
  },
  "name": "xml-object-stream",
  "description": "Low-memory streaming xml parser for node.js. Returns each node as an object. Uses node-expat",
  "version": "0.2.0",
  "repository": {
    "type": "git",
    "url": "git://github.com/idottv/xml-object-stream.git"
  },
  "main": "index.js",
  "scripts": {
    "test": "mocha -R spec --compilers coffee:coffee-script test.coffee",
    "prepublish": "coffee -c index.coffee"
  },
  "dependencies": {
    "node-expat": "~1.6.1"
  },
  "devDependencies": {
    "mocha": "~1.4.2",
    "coffee-script": "~1.3.3"
  },
  "optionalDependencies": {},
  "engines": {
    "node": "0.8.x"
  },
  "readme": "\nxml-object-stream\n=================\n\nStreaming parsers are hard to work with, but sometimes you need to parse a really big file. This module gives you the best of both worlds. You give it a specific node to look for, and it will return each of those nodes as an object, one at a time, without loading the whole document into memory at once.\n\nUses node-expat for fast(est) xml processing.\n\nInstallation \n------------\n\n    npm install xml-object-stream\n\nParsing a ReadStream\n--------------------\n\nLet's say we have a file, hugePersonDirectory.xml, that looks something like this:\n\n    <root>\n      <people>\n        <person>...</person>\n        <person>...</person>\n        <person>...</person>\n        <person>...</person>\n        <person>...</person>\n      </people>\n    <root>\n\nYou want to do something with each person object, but you can't load them all into memory at once. \n\n    xml = require 'xml-object.stream'\n    fs = require 'fs'\n\n    readStream = fs.createReadStream 'hugePersonDirectory.xml'\n    parser = xml.parse readStream\n\n    parser.each 'person', (person) ->\n      # do something with the person!\n\nThe parser emits some streaming events\n    \n    parser.on 'end', ->\n    parser.on 'error', (err) ->\n    parser.on 'close', ->\n\nYou can pause and resume it if the xml parser gets too far ahead of your processing\n\n    parser.pause()\n\n    # then when you catch back up\n    parser.resume()\n\nSince the parser takes any read stream, you can use it to parse urls without saving them to disk. \n\nNode Object Format\n-----------------\n\nNodes are converted to objects. For the following xml:\n\n    <person id=\"asdf123\">\n      <firstName>Bob</firstName>\n      <lastName>Wilson</lastName>\n      <employee id=\"asdf123\"/>\n      <note author=\"Joe\">Bob is a poor worker</note>\n      <note author=\"Jim\">Bob spends all his time parsing xml</note>\n    </person>\n\nYou can access attributes with the `$` property\n\n    person.$.id == \"asdf123\"\n\nYou can access the *last* child of a given name by its name. Text is accessed with $text\n  \n    person.firstName.$text == \"Bob\"\n\nNode names are available under the `$name` property\n\n    person.$name == \"person\"\n\nEvery child node is put into the `$children` array\n\n    notes = person.$children.filter (child) ->\n      return (child.$name is \"note\")\n\n    notes[0].$.author == \"Joe\"\n\nAPI Reference\n-------------\n\n    exports.parse = (nodeReadStream, [options]) ->\n      # returns a Parser\n\n    class Parser\n\n      # calls cb each time it finds a node with that name\n      each: (nodeName, cb) ->\n\n      # bind to 'end', 'error', and 'close'\n      on: (eventName, cb) ->\n\n      # pause or resume the read stream to let you processor catch up\n      pause: ->\n      resume: ->\n      \n\nOptions\n----------\n\n    {\n      # removes all namespace information from the node names\n      stripNamespaces: false\n    }\n\n    \n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/idottv/xml-object-stream/issues"
  },
  "_id": "xml-object-stream@0.2.0",
  "_from": "xml-object-stream@~0.2.0"
}
